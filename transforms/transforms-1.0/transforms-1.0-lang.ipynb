{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd55886-5f5b-4794-838e-ef8179fb0394",
   "metadata": {},
   "source": [
    "##### **** These pip install need to be adapted to use the appropriate release level. Currently used for testing alpha release of transforms 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c45c3c6-e4d7-4e61-8de6-32d61f2ce695",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install 'data-prep-toolkit-transforms[language]==1.0.0a0'\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c89ac7-6824-4d99-8120-7d5b150bd683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must enable nested asynchronous io in a notebook as the crawler uses coroutine to speed up acquisition and downloads\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a12abc-9460-4e45-8961-873b48a9ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from dpk_web2parquet.transform import Web2Parquet\n",
    "Web2Parquet(urls= ['https://arxiv.org/pdf/2408.09869'],\n",
    "                    depth=2, \n",
    "                    downloads=10,\n",
    "                    folder='downloads').transform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3df5adf-4717-4a03-864d-9151cd3f134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### **** The specified downloads folder will include the downloaded file(s).\n",
    "#import glob\n",
    "#glob.glob(\"downloads/*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7276fe84-6512-4605-ab65-747351e13a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:55:10 INFO - pdf2parquet parameters are : {'batch_size': -1, 'artifacts_path': None, 'contents_type': <pdf2parquet_contents_types.MARKDOWN: 'text/markdown'>, 'do_table_structure': True, 'do_ocr': True, 'ocr_engine': <pdf2parquet_ocr_engine.EASYOCR: 'easyocr'>, 'bitmap_area_threshold': 0.05, 'pdf_backend': <pdf2parquet_pdf_backend.DLPARSE_V2: 'dlparse_v2'>, 'double_precision': 8}\n",
      "10:55:10 INFO - pipeline id pipeline_id\n",
      "10:55:10 INFO - code location None\n",
      "10:55:10 INFO - data factory data_ is using local data access: input_folder - downloads output_folder - pdf2parquet-files\n",
      "10:55:10 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:55:10 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.pdf'], files to checkpoint ['.parquet']\n",
      "10:55:10 INFO - orchestrator pdf2parquet started at 2024-12-14 10:55:10\n",
      "10:55:10 INFO - Number of files is 1, source profile {'max_file_size': 5.308699607849121, 'min_file_size': 5.308699607849121, 'total_file_size': 5.308699607849121}\n",
      "10:55:10 INFO - Initializing models\n",
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 20015.24it/s]\n",
      "10:56:06 INFO - Completed 1 files (100.0%) in 0.847 min\n",
      "10:56:06 INFO - Done processing 1 files, waiting for flush() completion.\n",
      "10:56:06 INFO - done flushing in 0.0 sec\n",
      "10:56:07 INFO - Completed execution in 0.941 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dpk_pdf2parquet.transform_python import Pdf2Parquet\n",
    "Pdf2Parquet(input_folder= \"downloads\", \n",
    "               output_folder= \"pdf2parquet-files\", \n",
    "               data_files_to_use=['.pdf'],\n",
    "               pdf2parquet_contents_type='text/markdown').transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fef6667e-71ed-4054-9382-55c6bb3fda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### **** To explote the output from pdf2parquet, run the code below\n",
    "#table = pq.read_table('pdf2parquet-files/arxiv_org_2408.09869v5.pdf_application.parquet')\n",
    "#table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe8bf1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:56:09 INFO - pipeline id pipeline_id\n",
      "10:56:09 INFO - code location None\n",
      "10:56:09 INFO - data factory data_ is using local data access: input_folder - pdf2parquet-files output_folder - doc-chunk-files\n",
      "10:56:09 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:56:09 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:56:09 INFO - orchestrator doc_chunk started at 2024-12-14 10:56:09\n",
      "10:56:09 INFO - Number of files is 1, source profile {'max_file_size': 0.023062705993652344, 'min_file_size': 0.023062705993652344, 'total_file_size': 0.023062705993652344}\n",
      "10:56:09 INFO - Completed 1 files (100.0%) in 0.001 min\n",
      "10:56:09 INFO - Done processing 1 files, waiting for flush() completion.\n",
      "10:56:09 INFO - done flushing in 0.0 sec\n",
      "10:56:09 INFO - Completed execution in 0.001 min, execution result 0\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from dpk_doc_chunk.transform_python import DocChunk\n",
    "DocChunk(input_folder='pdf2parquet-files',\n",
    "        output_folder='doc-chunk-files',\n",
    "        doc_chunk_chunking_type= \"li_markdown\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d4f7bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### **** To explote the output from doc-chunk, run the code below\n",
    "#table = pq.read_table('doc-chunk-files/arxiv_org_2408.09869v5.pdf_application.parquet')\n",
    "#table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38480cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:56:59 INFO - exact dedup params are {'doc_column': 'contents', 'doc_id_column': 'document_id', 'use_snapshot': False, 'snapshot_directory': None}\n",
      "10:56:59 INFO - pipeline id pipeline_id\n",
      "10:56:59 INFO - code location None\n",
      "10:56:59 INFO - data factory data_ is using local data access: input_folder - doc-chunk-files output_folder - dedup-files\n",
      "10:56:59 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:56:59 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:56:59 INFO - orchestrator ededup started at 2024-12-14 10:56:59\n",
      "10:56:59 INFO - Number of files is 1, source profile {'max_file_size': 0.03043651580810547, 'min_file_size': 0.03043651580810547, 'total_file_size': 0.03043651580810547}\n",
      "10:56:59 INFO - Starting from the beginning\n",
      "10:56:59 INFO - Completed 1 files (100.0%) in 0.0 min\n",
      "10:56:59 INFO - Done processing 1 files, waiting for flush() completion.\n",
      "10:56:59 INFO - done flushing in 0.0 sec\n",
      "10:56:59 INFO - Completed execution in 0.0 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dpk_ededup.transform_python import Ededup\n",
    "Ededup(input_folder=\"doc-chunk-files\",\n",
    "    output_folder=\"dedup-files\",\n",
    "    ededup_doc_column=\"contents\",\n",
    "    ededup_doc_id_column=\"document_id\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27e36a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### **** To explote the output from eDedup, run the code below\n",
    "#table = pq.read_table('dedup-files/arxiv_org_2408.09869v5.pdf_application.parquet')\n",
    "#table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad27a462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:57:06 INFO - lang_id parameters are : {'model_credential': 'PUT YOUR OWN HUGGINGFACE CREDENTIAL', 'model_kind': 'fasttext', 'model_url': 'facebook/fasttext-language-identification', 'content_column_name': 'contents', 'output_lang_column_name': 'lang', 'output_score_column_name': 'score'}\n",
      "10:57:06 INFO - pipeline id pipeline_id\n",
      "10:57:06 INFO - code location None\n",
      "10:57:06 INFO - data factory data_ is using local data access: input_folder - dedup-files output_folder - langId-files\n",
      "10:57:06 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:57:06 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:57:06 INFO - orchestrator lang_id started at 2024-12-14 10:57:06\n",
      "10:57:06 INFO - Number of files is 1, source profile {'max_file_size': 0.031200408935546875, 'min_file_size': 0.031200408935546875, 'total_file_size': 0.031200408935546875}\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "10:57:08 INFO - Completed 1 files (100.0%) in 0.001 min\n",
      "10:57:08 INFO - Done processing 1 files, waiting for flush() completion.\n",
      "10:57:08 INFO - done flushing in 0.0 sec\n",
      "10:57:08 INFO - Completed execution in 0.036 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dpk_lang_id.transform_python import LangId\n",
    "LangId(input_folder= \"dedup-files\",\n",
    "        output_folder= \"langId-files\",\n",
    "        lang_id_model_credential= \"PUT YOUR OWN HUGGINGFACE CREDENTIAL\",\n",
    "        lang_id_model_kind= \"fasttext\",\n",
    "        lang_id_model_url= \"facebook/fasttext-language-identification\",\n",
    "        lang_id_content_column_name= \"contents\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c35cab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### **** To explote the output from langId, run the code below\n",
    "#table = pq.read_table('langId-files/arxiv_org_2408.09869v5.pdf_application.parquet')\n",
    "#table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e84ce78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:57:13 INFO - pipeline id pipeline_id\n",
      "10:57:13 INFO - code location None\n",
      "10:57:13 INFO - data factory data_ is using local data access: input_folder - dedup-files output_folder - doc-quality-files\n",
      "10:57:13 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:57:13 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:57:13 INFO - orchestrator docq started at 2024-12-14 10:57:13\n",
      "10:57:13 INFO - Number of files is 1, source profile {'max_file_size': 0.031200408935546875, 'min_file_size': 0.031200408935546875, 'total_file_size': 0.031200408935546875}\n",
      "10:57:13 INFO - Completed 1 files (100.0%) in 0.003 min\n",
      "10:57:13 INFO - Done processing 1 files, waiting for flush() completion.\n",
      "10:57:13 INFO - done flushing in 0.0 sec\n",
      "10:57:13 INFO - Completed execution in 0.003 min, execution result 0\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from dpk_doc_quality.transform_python import DocQuality\n",
    "DocQuality(input_folder='dedup-files',\n",
    "            output_folder= 'doc-quality-files',\n",
    "            docq_text_lang = \"en\",\n",
    "            docq_doc_content_column =\"contents\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d98b854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### **** To explote the output from Doc Quality, run the code below\n",
    "#table = pq.read_table('doc-quality-files/arxiv_org_2408.09869v5.pdf_application.parquet')\n",
    "#table.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
